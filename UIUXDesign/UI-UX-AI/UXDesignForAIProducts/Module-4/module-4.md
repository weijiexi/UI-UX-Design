# MODULE 4: User Trust and Explainability in AI Systems
We will learn how brittle user trust in AI can really be, and how that trust can significantly impact the adoption of generative AI models. We will explore how user trust impacts the way these models are viewed and learn about the concept of complementarity in the field of human-computer interaction.

- **Algorithm aversion**: Tendency for people to distrust and avoid using algorithms after observing them make mistakes.  
- **Complementarity**: Refers to the collaborative relationship between humans and AI systems where both contribute their unique strengths to achieve better outcomes than either could alone.  
- **Local Interpretable Model-agnostic Explanations (LIME)**: Explainability technique focused on explaining specific generative AI decisions by trying to convey what would have moved a particular input to a different outcome. This means that the model is not completely describing what makes it decide the way it does, but it does explain what would have made it provide a different answer in case it got it wrong.  
- **Mechanistic interpretability**: Explainability technique focused on understanding the fundamental behaviors of algorithms by analyzing patterns of neuron activations. It helps to ensure that the AI model is learning and making decisions for the right reasons.  
- **Overreliance**: A situation where users depend excessively on AI outputs, often accepting them without sufficient scrutiny, even when the AI's suggestions are incorrect.  

<details>
  <summary>User Trust in AI and Why It Matters</summary>

  ### The Importance of User Trust in AI
  -  designers need to identify those situations 
     - **where artificial intelligence should be responsible for making certain actions easy (accelerators)** 
     - **where it should double check the person's actions (verifiers)**
     - **where it should lay out a set of options (design galleries)**
     - **where the AI should act without any human intervention at all**
        - For example, we would not want humans in charge of deploying a vehicle’s airbag. This should be an automated task instead. 
        - Whereas when wording an email, you'd certainly want to be able to make final edits before sending to a recipient.
            - To achieve complementarity, we need to focus on situations where there is a legitimate possibility that a human-in-the-loop (or AI-in-the-loop) design can yield benefits. Airbags don't fit. But, code development and strategic decision making might.

- **How do we design AI systems that project clearly what they can and can't do, so that people retain trust in them?**

nonthreatening AI is trusted better: it's that the AI projected a level of competence and importance that was aligned with the level of trust that doctors were willing to place in the AI.
</details>


<details>
  <summary>Aiming for Complementarity</summary>

- Each modality adds to the insights and knowledge of the other, resulting in well-rounded results with reduced information gaps.
- The hope is that the AI can help accelerate me or identify my errors, and I can help identify the AI's errors too. 
- If we play this duet successfully, the final result is better than anything either modality could have produced alone.
**complementarity can be achieved** , but not everyone achieved complementarity. **It actually had minimal impact on experienced and highly skilled workers**. Why? Well, because the AI had been trained on those experienced workers, so it was just recommending what they would have done anyway!
- In areas like creative product innovation, ideation, and content creation, approximately 90% of participants using generative AI improved their performance, with an average productivity increase of 40% from those who did not.
</details>



<details>
  <summary>Algorithm Aversion and Overreliance</summary>

### Explainability Techniques
### Simplifying the Model
### Mechanistic Interpretability
### The Explainability Dilemma
</details>
