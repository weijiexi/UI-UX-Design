
# Full Transcript ‚Äì AI and Human-Centered Design Meeting
## Date: July 1, 2025


<details>
  <summary>09:08:40</summary>

A recent MIT meta-analysis examined over 100 studies asking: **Does AI improve human performance in real tasks?**  
Surprisingly, the answer is often **no**‚Äîespecially **for decision-making tasks**.

## Core Insight

**In many cases, people + AI performed worse** than either people or AI alone. This tends to happen when the **type of task** is not well-suited for human-AI collaboration.


## Problem Types: Rough vs. Sharp Edges

### Sharp-Edged Problems
- Only one correct answer
- Examples: fraud detection, medical classification, spam filtering
- AI is either right or wrong
- Humans **over-trust confident AI outputs**
- **Design risk**: errors can be costly and misleading

### Rough-Edged Problems
- Many acceptable solutions
- Examples: writing, coding, design
- AI gets the user 80% of the way; human finishes
- **Lower risk**, more room for creativity
- Works better with current generative AI systems

---

## Design Recommendation

> "If possible, **turn a sharp-edged problem into a rough-edged task**."

- Example: Instead of having AI predict construction flaws (sharp), have it surface **possible weak points** and let a human engineer review them (rough).
- This reframing increases the chances of **beneficial human-AI collaboration**.

---

## Takeaways for Designers

- **Generative AI thrives in ambiguous, creative spaces**
- **Human-AI integration needs careful interface design**
- **Avoid blindly trusting AI in high-stakes, sharp-edged contexts**
- Reframe tasks to reduce the **impact of inevitable AI errors**
- Augment human capability, **don‚Äôt replace it**

---

*‚ÄúIt‚Äôs not that AI is worse at sharp problems. It‚Äôs that humans are less forgiving of errors in high-stakes decisions.‚Äù*
"""

<details>
  <summary>09:08:40 transcript</summary>

Ai over-reliance. Um, and if you haven't caught this yet, we will be doing it soon. And this is a big debate about whether AI makes us better at what we do. And there was this piece of work. That came out, I think, the end, yeah, end of 2024, so maybe a little over 6 months ago. Where they're looking at. From MIT, where they're looking at. What's called a meta-analysis. Of all these studies where they give people AI and see whether it makes them better at their jobs.And so they're gonna compare. The person with AI. To whichever one is better, the person alone. Or the AI alone. It looks like the image isn't working? Let me stop and reshare. That's weird. I've never had that problem before. Let's see if that helped.

Um. The‚Ä¶ that's so weird. Um‚Ä¶. Uh, do we blame Zoom, or do we blame Apple? I'm not sure. Uh, probably Zoom. The‚Ä¶. So, what they started‚Ä¶ so they looked at all these studies of.

People with AI, and‚Ä¶. And they produce over 100 studies, and they produce the following. This is a graph. In this graph.I'm gonna help you walk through it for a moment. The vertical line, the big‚Ä¶ at zero.Is, uh, says that in this study. The‚Ä¶.
People plus AI did exactly as well, no difference. Then people alone are AI alone, the better of people alone are AI alone.
Each sort of little horizontal line in this graph is one study that they picked up from the academic literature.
Anything that's in the red, to the left of zero, means that people plus AI did worse.
Then people alone or AI alone. Anything to the right of zero in the green.
Is that people plus AI did better than people longer AI alone.
So I hope what you're gathering here. Is that, on average, people are performing worse with AI.
Then the better of humans or AI alone. That is wild.
I sort of read that as people throwing AI at things without having designed well.
How it's supposed to integrate. Into that task, or into that job.
And I want to lay out. The authors do a bit of analysis that I think helps us make sense of this.
The stuff that was most likely to be in the red.
Or what are, like, called decision-making tasks, so these are, like, classification tasks. Like, is this‚Ä¶.
Person gonna get readmitted to the hospital in the next month, you know, given.
The following, uh, you know. Comorbidities and other kinds of things. And the things that were most likely to be green, those‚Ä¶ the studies were content creation, so, like.
Coding, writing, you know, copy, uh, all these kinds of things.

And‚Ä¶ so‚Ä¶. I have a theory about what's‚Ä¶.
Going on. What I think what is going on‚Ä¶.
Is that, um‚Ä¶. The‚Ä¶ I'm gonna put it this way.
I want to draw a distinction between what I call rough-edged problems.
And‚Ä¶ sharp-edged problems. I'm claiming that, basically.
There's no difference in the underlying quality of the AI between these kinds of problems.
But‚Ä¶. Given an equivalent capability.
Generative AI sort of works sooner, faster. Proponents with rough edges than sharp edges. What do I mean? Okay.
A rough-edged problem is a problem with many correct solutions. Writing, drawing, playing Mario.
Coding, all those kinds of things. A sharp-edged problem.
In contrast, is where there's only one correct solution. Agentic tool use, decision problems, prediction problems.
Like, spam filter. Classic sharp edge problem. You're either right or you're wrong.
My claim here, I think of the sense that I make of this, as this literature starts to‚Ä¶ it keeps advancing.
Is it's not that the AI is inherently better or worse.
**For rough and sharp-edged problems. It's that, actually, it's on our side as humans, our tolerance for error is very low**.

In sharp-edged problems. And the‚Ä¶.
So, just think of it this way. Shortage problem, I mentioned spam filter.
Spam filters have been around since the 90s. But it's only been in the last couple years, I think, that the spam filter has been good enough that I haven't had to check my spam folder, like, literally every day, because I would miss some really important.
You know, email from my boss or something like this. And if you take it back to this.
Um, there's a question from‚Ä¶. From Susan, aside from the one about Sonic, who is quite cool. I actually‚Ä¶ I was a Sega kid growing up.
Um, if you Google this. Title, you should be able to find the paper, it's open access.
So, when they say that the biggest losses are for decision-making tasks and the biggest wins are for content creation tasks.
I think what's really going on is about rough versus sharp-edged problems.
So, um‚Ä¶. I think what's going on here is that a lot of the red ones are sharp-edge problems, where essentially the AI's either right or it's wrong, and as we'll talk about in the course.
When it's wrong, it sort of still influences us, and we're like, yeah, it might be right, and you go with it. Whereas with a rough-edge problem, the AI can get you 80% of the way there.
And then you can carry it the rest of the way, and get something that's better. So just to pause for a second, I think, like, the sense I make of this from a design perspective.
Is that if you can, at all possible. If it's at all possible, turn a sharp-edged problem. If, like, if the‚Ä¶.
If leadership is, like, giving you a sharp-edge problem, like, oh, create a, you know, AI augmented solution for, whatever, detecting.
Flaws in this, you know, construction material or something like that.
Try to turn it into a rough-edged problem. And you're going to see much‚Ä¶.
Bigger benefits much sooner. Which is to say, oh, okay, well then what I'm gonna do is I'm gonna use the AI to sort of spit out a bunch of potentially relevant information, but it's not going to make the‚Ä¶.

The prediction itself is just going to tell me what I ought to be paying attention to, and let me sort of follow through from there.
Trying to transition, I think, from sharp edge to rough-edged is gonna be a pretty useful design technique going forward. Um‚Ä¶.
A rough, uh, excuse me, I shouldn't use that term. A way to think of it would be‚Ä¶.
There's, like, a circle of the things that AI, I think, can do sort of fully autonomously right now.
Let's call those the sharp-edged problems, right? I don't have to oversee it.
Then there's another set of‚Ä¶ there's like a ring outside of that, which is‚Ä¶.
Problems that people with AI can solve. Effectively, and I'm gonna call that rough-edged. Like, if the AI can get me 80% of the way there, I can finish it.
I think that's a‚Ä¶ that's the space of rough-edge problems. I think that's where design can really flourish.
Is in that space of rough-edged problems. So, uh, I want to stop talking for a minute, because I'm more interested in hearing what you all have to think.
Um, the. So, I would invite you in the chat, or raise your hand in Zoom. I want to talk a little bit about this. Have you‚Ä¶.
Run into these kinds of situations? Does this resonate? Does this not resonate? And you think, I'm totally just blowing, you know, hot smoke here. What's going on?
</details>
</details>


<details>
  <summary>09:16:37</summary>

The discussion focused on the challenges and strategies in using AI for **high-risk and safety-critical environments**, like fraud detection, moderation, and banking. Participants explored the practical tension between **rough-edged** and **sharp-edged** problems and how organizations manage AI's limitations in real-world applications.

## Key Concepts

### Sharp-Edged Problems
- Only one correct answer (e.g., fraud, moderation, prediction).
- AI makes confident errors, which are hard to detect.
- False positives have serious consequences (e.g., blocking legitimate transactions).
- Organizations like banks build **tiered response systems** to triage AI decisions based on confidence levels.

### Rough-Edged Problems
- Multiple acceptable outcomes (e.g., data querying, writing scripts).
- AI is helpful in providing draft solutions users can refine.
- Designers can convert sharp problems into rough ones for better collaboration.

---

## Discussion Highlights

- **Ashley** emphasized the importance of scale: AI handles massive data efficiently, but risk must be carefully managed.
- **Georgie** shared how moderation AI is tuned to be overly strict to avoid violating rules‚Äîa trade-off between **false positives and platform safety**.
- AI is built to **make mistakes**, and design must account for that fact.
- Designers must move away from assumptions of ‚Äúperfect AI‚Äù and instead focus on **transparency and user oversight**.

---

## Design Strategies

- **Triage Models**: Use AI for high-confidence outputs; defer unclear cases to humans.
- **Prompt Engineering**: Carefully craft AI prompts to reduce risk.
- **Cognitive Forcing Functions**: Have users make their own judgments before seeing AI suggestions to prevent blind acceptance.

---

## Insightful Quote

> "Designing for AI means designing for error‚Äîbecause it *will* be wrong, and it won't always know when."


This conversation reinforced the need for thoughtful AI integration that emphasizes **collaboration over automation**, **control over convenience**, and **trust over speed**.

<details>
  <summary>09:16:37 transcript</summary>

Someone's gonna have to bite the bullet and be the first one.Yeah, Ashley, thank you.I'll buy a bullet. Um‚Ä¶. Congratulations again for, uh, becoming full professor. That's really exciting.

09:16:45
My mom just says congratulations, so now I'm feeling really good.
09:16:48
Yeah, I‚Ä¶ you and I both‚Ä¶ I‚Ä¶ I talked to my mom at least once a day, and we're texting constantly, so‚Ä¶.
Yeah, here's to all the people who talk to their moms.
Um, I‚Ä¶. I think in general.
Just in general, what you've said about. Um, rough versus sharp edge problems works, um, but when you get into, um.
For example, the fraud space. Um, fraud can be.
I think‚Ä¶ I guess it's‚Ä¶ it's sharp in terms of it‚Ä¶.
If there's a transaction, it either is or is not fraudulent.
Um, and one of the ways that AI helps in situations like that.
Um, just one of the many ways is scale. So, um‚Ä¶.


09:17:43
It's not‚Ä¶ I mean, it wouldn't necessarily be for the‚Ä¶. On the consumer or customer end, although there are aspects to what I've worked on that are on the consumer-customer end, it tends to be more.
The back end, like on the bank sides. But when you have‚Ä¶.
Not just sharp-edge problems, but problems. That have very severe consequences, if not handled correctly.
09:18:10
Yes.

09:18:10
And those are at scale. Ai can help with the.
Scale. So, like I said, I think, in general. I think it's just, like, kind of a good generality.
But then when you start thinking about specific applications. And specifically in, um.
Safety-critical or high-risk, or highly regulated environments. That's where you get‚Ä¶ I see we start getting caveats, which is‚Ä¶.
We have masses amounts of data, it's really important that we see the red flags, and we need AI to help us with that.
So that was my‚Ä¶ my point. That's all.

09:18:45
Right. Yeah, so there are a couple things embedded in that. I think one is‚Ä¶.
That your tolerance for risk. Will impact whether‚Ä¶ I think even you can deploy AI. So, like, yes, AI helps you at scale.
But imagine that, um, I don't know, 1 in 10 AI.
Classifications were false positives. So you were, like, denying, you know, credit card transactions.
Um, a lot that we're actually not fraudulent, then your customers would also get really upset.
And, um, and so as a result, I think a lot of companies.


09:19:24
Have to do this very complex calculus of, like, what is the sort of cost of a false positive to us?
And how did we trade that off against the scale? So, um, you know, a lot of‚Ä¶.
Tech companies have to deal with this, too. The titles are often different, they often live in what's called integrity.
Teams where they're trying to get rid of fraudulent accounts, or, you know, social media manipulation.
And so, you know, it'll often use some sort of tiered thing, where if, like, the AI is very confident, it just sort of actions on it. If it's sort of a yellow light, it'll route it to human review, and if it's a red light, or sorry, if it's a green light, I got the lights mixed up.
Greenlight, it's fine, it'll just sort of let it through. And I suspect that a lot of‚Ä¶.
Yo, if you are in a space where ultimately there's no way to turn the sharp-edge problem into a rough-edge problem, I feel like that's sort of the most common.
Triage technique is essentially to have the AI cover the 95% of the cases that are just really obviously clear.
And then, sort of flagging the other 5%, where it says, well, here's what I'm thinking about.
And then letting human review handle that, and then you toss that into the training data.
So, yeah, I mean, I agree, but I view it mostly as a result of the, sort of.


09:20:34
The loss‚Ä¶. Like, the risks that you take on.
In banking, like, the downside risks are often so huge. That, like, you make different decisions than if it's, like, a consumer app.

09:20:46
Where it's like, okay, it's fine if it, like, mistrans‚Ä¶ you know, mistranscribes me occasionally.

09:20:50
Yep, and just in‚Ä¶ if anyone has banked at Chase and wonders why.
They get so many transactions stopped. This is why Chase has really, uh, really conservative, uh, calculations.
When it comes to fraud and risk. Um, it tends to make Chase a little safer.
Or a lot safer than a lot of other financial institutions.
But it also stops a lot more payments, and many of them are false positives, so‚Ä¶.

09:21:18
Yeah. Yeah, I, uh‚Ä¶. Y'all should keep your hands up if you're interested. Like, Georgie, were you no longer interested?

09:21:27
Yeah, I can go. Thank you, thank you for this, like, uh, the idea. I haven't posed in such a way, like, rough‚Ä¶ roughage problem, this is‚Ä¶.
Uh, it was this verse of sharpitch problem, and I felt the same, actually, because I work as a product designer, and I have, like, a lot of working with data, and while I'm working with data, I need to write a lot of code to retrieve data from SQL and so on.
And different, uh, different syntaxes using. And I feel why I'm preparing a script, and it's definitely rough, uh, roughage problem. I need to retrieve my data, and I can just.
Tune my script to‚Ä¶ to get the data that I'm really interesting about, and do, like.
Better decisions based on the data that, uh, that‚Ä¶. Ai can help me retrieve, but on the other hand, where I have, like, an AI feature implemented in a product that I'm working on, and here's, like, the, uh, definitely a sharp edge problem.
Which is, for example, a moderation, and because I'm working on a communication platform, and for a B2B segment, so the all-employee can type whatever they want, and as a.
Safe‚Ä¶ safe feature for admins, for, uh, for CO, and for example, uh, for HR, there's, like, a moderation feature that prevents from boosting, for example, personal data. Ai, just understands.
And here, actually, I feel that we're right now building process around false positives, because we are, uh, we are‚Ä¶.
Tune our prompting in a way to build more false positives, rather than, uh, rather than.
Flood, uh, flood with a‚Ä¶ with a possible violation of rules in the platform, because it can lead to problems.
And here we just communicated with the clients in such a way that, uh, we had‚Ä¶ we more it, like, in a stricter way, prompting to generate more false positives, to not let, like, the real positive stuff.
Go to the platform and be available to everyone and just violate some rules, and I don't know, leak the data, of course, to clients, customers, and so on.
So, that's kind of interesting how the‚Ä¶. Probability of errors.
Choose how we build processes inside‚Ä¶ inside the platform, yeah.

09:23:41
Well, I think what's unique about designing for AI is that.
It will make errors. You have to design around not only the, like.


09:23:50
But, like, it might, but that it will. Like, AI, by definition.
Unlike basically any other tool you've used, you're, like, using a tool that is gonna make mistakes.
Right? And those mistakes can come for many different reasons. It might be because, you know, it wasn't in the training data.
But it might also just be because, like, it's an uncertain outcome.
Um, and‚Ä¶. I don't think we have great design patterns for conveying that right now.
This is‚Ä¶ you know. The people's usual first stop, and we'll talk about this in the course a bit, is to, like, just sort of, like, add confidence intervals, or, you know, sort of‚Ä¶.
Confidence indicators. But unfortunately, there's no way to really calibrate these things. The problem is that when the model is wrong.
When it's really wrong, it often doesn't know. It's like, they're often confidently wrong.
Um, you may remember a number of years ago when, like, you know.
Alphago beat the world's grandmaster at the game of Go. And, like, most of the time it was just.
Totally, like, defeated. It was, like, not‚Ä¶ no contest. But there was one game where it really‚Ä¶ you know, he beat it really handily, and he did it by making some move that was really wild, so weird that it had never seen it in its training data.
And then AlphaGo had just had‚Ä¶ was pushed over to some weird part of its probability distribution and just started.
Making effectively random moves. Um, it'd be like, you know, those cartoons you watched when you were a kid, when, like, you asked the robot something, like.
Logical, like, uh, something weird and just sort of, like, blows steam out of its ears. That's basically what happens, except your AIs will do this too, it'll just not be obvious.
Um, and so‚Ä¶. How do we‚Ä¶.
How do we design around an assumption that the AI will be wrong and it won't know when it's wrong?
I think that usually the sort of first idea is, like, oh, well, we'll just have a human oversee it.
But as we'll talk about in the course, often people sort of are too willing to go with what the AI suggests.
And so. The only sort of surefire way to do it is to have these sort of, like, cognitive forcing functions, as they're called, where you, like.
Have the person first guess on their own, then you unveil what the AI says, and then you ask them to make a final decision, or something like this. But it's‚Ä¶ that takes away a number of the benefits of using AI in the first place. That's why I'm suggesting try to, like, turn things into rough-edged problems, because then.
Sort of, I can use it as a starting point and then continue.
</details>
</details>

<details>
  <summary>09:26:23</summary>

**üë§ Speaker:** Jesse (Cybersecurity Designer)

## Design Challenge:
In cybersecurity, most tasks are **sharp-edged problems** ‚Äî users want exact answers (e.g., detecting threats in a network).


## Jesse's Design Strategy:
- Instead of trying to make the AI solve everything exactly, the team is **reframing the sharp-edged problem into a rough-edged workflow**.
- The AI **suggests potential threats** and labels them with uncertainty.
- Users are informed: *‚ÄúThis is an AI-generated result, and may contain false positives.‚Äù*
- If the user disagrees, they can **dismiss it with a click** ‚Äî this feedback is then used to improve the AI model.

## Why This Works:
- It embraces the **limitations of AI**, rather than pretending it‚Äôs always right.
- Builds **user trust** by:
  - Explaining the AI‚Äôs role clearly
  - Allowing user feedback
- Turns rigid classification into a **collaborative detection process**.

## Key Design Principles:
- **Explainability**: Users know what the AI is doing and why.
- **Human-in-the-loop**: Users validate and correct AI suggestions.
- **Learning from feedback**: Dismissed suggestions help retrain the model.

## Instructor Response:
- Agreed with the approach as a **practical and realistic way** to handle AI's imperfections.
- Emphasized **pragmatism over hype**, highlighting that AI design today is about making the most of existing tools with **clear, user-centered interactions**.

<details>
  <summary>09:26:23 transcript</summary>
Jesse!
Okay, so I‚Ä¶ Heidi resonates on everybody's saying, uh, I work in cybersecurity, and I'm currently designing an AI tool.
And in cybersecurity, most of the problems are actually sharp-age problems. So for users.
People actually wanted to find the needle on the haystack. So they wanted to find the exact threat or attack that's happening in their network.
But then for the AI tool, we're actually‚Ä¶ I don't know if that's‚Ä¶ we're trying to turning ShopReach into raw fish onto, Professor, you said this is a kind of, like, a design.
Direction that I realized, actually, we're trying to turn this sharp age problem into a‚Ä¶.
Into a raw fish problem. Um, and then, so we're giving users a lot of suggestions, like, these are potential risks in your network, these are‚Ä¶ this might be attacks.
A lot of them are actually false positive, and then what we have designed‚Ä¶ I mean, we're still trying to do a lot of user testing and user research. What we're trying to do is, we make AI.
Kind of, like, uh, explainable. Like, in the beginning, we clearly tell the user this is the result from AI. And so user understand that AI sometimes might give you a false positive result.
And then we ask the user for their feedback. Like, if they see this is a false positive.
They‚Ä¶ there's a little button they could click to dismiss it, and then we take that as a‚Ä¶.
As a result, we could use it to help us to improve our AI model. So that's‚Ä¶ that's what we're doing right now.
For the AI‚Ä¶ AI products we're designing in our company. Um‚Ä¶.
Nothing, nothing, nothing special or creative, just wanted to say, like, this is exactly what we're using impractical.

09:28:16
Yeah, I mean, I‚Ä¶ I‚Ä¶. I don't really actually have strong comments on that. I mean, it seems like a reasonable approach. I'm curious if others have sort of aligned on a similar approach?
I think, like, it just seems like, sort of, the practical right way to think about it in many ways.

09:28:40
Yeah, I mean, people love to proclaim, like, from the rooftops, these huge, like.
Grand theories of AI and how it's gonna, like, you know, whatever, we're gonna hit AGI, blah blah blah. I'm actually‚Ä¶ I'm kind of a‚Ä¶ not a skeptic, I'm just much more of a pragmatist, I'm just like, well, here's what we've got today, and here's how we design with it.
So, I agree with your approach. Ivy, do you want to jump in?
</details>
</details>

<details>
  <summary>09:29:02</summary>

**Does the use of AI for ambiguous, rough-edged problems contradict the original purpose of automation‚Äîoffloading precise, rule-based tasks?**

## Key Insights

### 1. **Shift from Automation to Augmentation**
- Ivy questioned whether designing AI for rough, ambiguous problems goes against the traditional idea of automation.
- The instructor responded that while early AI hype emphasized full automation (e.g., replacing radiologists, coders), this was largely **overhyped**.
- Instead, the focus should shift to **augmenting human capability**‚Äîsupporting and enhancing what humans already do, not replacing them.

### 2. **Replacement vs. Empowerment**
- **Some rote jobs may be replaced**, but the most exciting opportunities lie in augmentation.
- Augmenting user workflows can lead to tools users **don‚Äôt want to give up**‚Äîa more powerful emotional attachment than automation.

### 3. **AI Design Should Be Embedded, Not Just Chat-Based**
- Many current products wrap a chatbot around a feature‚Äî**a weak design choice**.
- Stronger integration involves **embedding AI in traditional GUIs**, allowing for layered, context-aware assistance.

### 4. **Prompting Quality Impacts Output**
- Another participant shared how prompting quality (more context, better parameters) dramatically affects AI response quality.
- **Training users to prompt well** and understanding that AI needs structured inputs can improve performance.

## Takeaways for Designers
- Think in terms of **augmentation, not automation**.
- Design for AI **error and uncertainty**.
- Help users provide **richer prompts** and **meaningful parameters**.
- Move beyond chatboxes‚Äî**layer AI within traditional tools** for deeper functionality.

*‚ÄúThe future of AI design lies not in replacing humans, but in giving them superpowers.‚Äù*

<details>
  <summary>09:29:02 transcript</summary>

Sure, yeah. My name is Ivy, and I really interested in the concept that you just mentioned about ROFA and ShopEdge. I did put the question in the chat, but I just wanna, um‚Ä¶.
Speak out loud here. Um, so my question is, does this contradict with the original goal of.
Automation. So, which aim to offload pre-sized, repeatable, and rule-based tasks.
If AI is better suited for ambiguity or approximation, how do we reconcile that with the foundation ideas behind automation?

09:29:41
Yeah, I think‚Ä¶ no, it absolutely contradicts that, but I never thought that that was the original goal. Maybe other people did.
I do think it got a lot of, um‚Ä¶. Airtime, so to speak. I think Jonathan was saying something similar in the chat.
Where, uh, you know, it's like, he would be like, you may have heard, oh, AI's gonna replace radiologists, AI's gonna replace coders, AI's gonna replace designers, like, everyone's replaced, there's no one left.
Um, I think that this is‚Ä¶. More hype than substance in many ways. And‚Ä¶ and I think that it's‚Ä¶.
Both, sort of, wrong and pragmatic terms, in terms of what I‚Ä¶ or predictive terms of what I think will happen, and wrong maybe in terms of‚Ä¶ in normative terms of, like, what should happen.
So, I think that in most cases‚Ä¶ so, first, I need to acknowledge, I think there will be.
Replacement and displacement. Like, AI will replace some jobs. I think that seems inevitable. I don't know that I can predict strongly which ones those will be.
Um, but certainly jobs that involve. Very few tasks that are all extremely rote.
Seem the most likely. Um, that said, I think the broader transformation that no one is paying attention to, or not enough people are paying attention to, is the sort of broader.
Augmentative transformation. You know, that's why I sort of put this whole unit in the course, is like.
Trying to orient the designers around, well, what is the augmentation solution here? In part because if you try to replace, then you get all of this organizational friction and pushback.
Um, and in part because, you know, if you augment, you give people superpowers, they will, like.
Say, you know, pry your product out of my cold, dead hands kind of thing. You want that kind of emotional reaction to your design.
Um, and I think that, honestly, in most cases. You know, augmentation is the thing that winds up being transformative.
Right? Let's think about prior technologies. It wasn't‚Ä¶. Just that it took‚Ä¶ like, yes, it took over‚Ä¶.
Things that we've been doing in other ways, but then it opened up this much broader set.
Of uses that we had never previously imagined. And those are all augmentative.
Like, okay, my hot take, basically, is that I think we're in the MySpace era.
For AI, right? We have this interesting technology. It is clearly capable of something.
But people have not figured out the genre. And it's‚Ä¶ I think that the‚Ä¶.
You know, people are just all rushing to study, like, the technical bits of AI, and look, they're in my department, I teach these people too.
But I think the transformative bit is gonna actually be at the app layer.
In the next 5 years. Like, I think that there's just gonna be this huge opportunity to redefine what people do with AI, how that augments us.
That I really want to keep‚Ä¶ try to keep our focus on.
That's, again, my hot take. Um, let me jump over to the‚Ä¶.
Chat. There's some discussion of MCP, model context protocol. Um, I think‚Ä¶.
Potentially. I mean, the vision of model context protocol is that, you know, sort of any app can send.
You know, information and queries over to, you know, a chat GPT, right? So, in principle, the promise of this is that it can become much more integrated.
With a lot of different applications, and I think that is going to push us in interesting‚Ä¶.
Waze design-like. I do think‚Ä¶ okay, here's another hot take. Um, most people think that designing with AI right now is wrapping a chatbot text box around their product.
I think this is stupid. Um‚Ä¶. And instead, often this AI will be transformative, but I think it's not gonna look like that. I think it might be, like, layered inside of the system, and we may still have very traditional GUIs.
For the interaction with these things, but it's‚Ä¶ they're just going to be much‚Ä¶.
More, sort of, extensive in what they can do. When designed correctly. Um‚Ä¶.
So, Omar points out in the chat, has it been replacing jobs and coding customer service? Um‚Ä¶.
That's a good question. I actually haven't seen strong evidence. I think some companies have done this, and then some of them rehired afterwards.
Um, I've not seen‚Ä¶. I will say‚Ä¶ okay, so on the education side, look, Stanford is not a very average university.
Our students in CS are still getting entry-level coding jobs. They are having to race a little bit more to get them. They're not, like, falling from the sky like they were 10 years ago.
Um, but I haven't seen‚Ä¶. Sort of large-scale layoffs, as much as the CEOs keep saying that this is going to happen.
I would love to have that kind of conversation. Um, the‚Ä¶ Okay, Joey, I think, has been waiting. Shall we jump to Joey?

09:34:36
Hey, how's it going? Thanks. Um, two questions, uh, one question quick. Um, there's a task in my assignments about propose a design concept for AI Power. Is that‚Ä¶.
Related to the capstone, or just its separate project altogether? I just didn't know if I should wait to do that to get more‚Ä¶.
In depth here, or does that make sense, or‚Ä¶? Good question. It's called the, uh, the assignment is called Propose a Design Concept for an AI Powder Interactive Product. Is that‚Ä¶.

09:35:02
Related to our capstone project, like, the beginning of it? Can we hold off on doing that right away, or, like, can I just‚Ä¶ or should we‚Ä¶.
09:35:04
Yeah, I think that's the first step here. Let me‚Ä¶ I actually have a backup‚Ä¶.
So there is a‚Ä¶ yeah, yeah, no, here, I'm just throwing up the whole thing so you can see, sort of, the arc of the project. Um, the first part will be‚Ä¶.

09:35:18
To‚Ä¶ so the course isn't itself end-to-end isn't very long, so‚Ä¶.
We‚Ä¶ you might need to‚Ä¶. Stick with the timeline that they're giving you if you want to have time.
We're not implementing this, um, just to be clear, so it's like, part one is basically to do what you'd normally think of as sort of, like, need finding or problem identification, um.
You know, what is it‚Ä¶ what do you think might be augmented with AI? And the second part is going to be, essentially, you know, you're going to be using Figma or other tools, um, to prototype what this is going to be like.
Um, given the short‚Ä¶. Length of the course, we're not going to do. Um, normally what I would do with my students is a round of, sort of.
User feedback and iteration. Here, I actually‚Ä¶ due to the, kind of, construction of the course, I wanted the third part to be around sort of like a‚Ä¶.
Um, and ethics and societal Impact Review. Uh, like, when other people start using this, how's it going to impact things? So, this is roughly how this is going to lay out.
So, I would encourage you to, yes, just jump. In. If you feel like you need more time, I‚Ä¶ I mean.
I'm not Enrique, I don't‚Ä¶ I'm not global alumni, so I don't know exactly how this‚Ä¶ the timeline works, uh, in the sense of, like, if you.
Turn it in late, what happens? I assume they don't send the Spanish Inquisition after you, but it might delay the feedback.

09:36:40
No, that makes sense, thanks. Yeah, I was just‚Ä¶ I didn't know if, like, I had to think about the whole project, like, what I was gonna research a little bit and talk about this, or it was totally separate, so I'll just make sure I do it now, but uh‚Ä¶.

09:36:50
Knowing that this is part of the capstone, and I gotta think about if it's‚Ä¶.

09:36:54
Something I want to go all the way through. Gotcha. Okay, and sorry, just to add to the Sharp Ruff really‚Ä¶.
Great thinking, I can help the team out. We've been struggling a little bit designing products. One of the things‚Ä¶ I don't know if it's related, but might is‚Ä¶.
I guess I didn't know at the time, but the roughs‚Ä¶ the rough side of things were struggling with.
The users, either internally our teams or consumers that we build for.
We're noticing that we're struggling really bit with the rough side when it's not, like.
Sharp, meaning‚Ä¶. They don't‚Ä¶ the users don't know the parameters to do, and it makes a big difference on what AI.
Gives you as a solution. For example, you know, if you're, uh.
A designer, and you say, hey, I want to‚Ä¶ I want this pattern, can you give me some examples? And, uh, I want to see if it's compliant for accessibility.
They don't understand it. Well, you have to say what country you want it to be compliant for.

09:37:39
And, like, what's that‚Ä¶ you know what I mean? They don't, so‚Ä¶.
We're always getting these false positives, but at the same time, when we actually start talking to teams.
We realize that a lot of the‚Ä¶ we can get a better success rate.
On the answers from AI. When we help them and educate them with, you know, training.
On how to prompt better, and how to do better parameters. Like, you can't just assume AI knows who you are.
Like, you have to give it a lot of parameters, or even constraints, like the person you are, what's the category you're in of industry, like.

09:38:06
Yeah. Yeah, yeah, I mean, I‚Ä¶ okay, so broadly, I agree. I actually‚Ä¶ I want to have a longer conversation about that, so I want‚Ä¶ can I defer my thoughts on that for just a moment, and then maybe we'll talk?

09:38:10
And the more we notice, the more‚Ä¶. Parameters that our users are giving.
The better AI is providing a more, like. A better solution for. And I don't know if it's real‚Ä¶ I'm really interested, and I'll definitely test it with the teams, but uh‚Ä¶.
And I'll chat with the teams about this, you know, let's see how this works when we‚Ä¶.
Really prompt a little bit better, do multiple prompts, go in deep, right? Go in deep, don't just‚Ä¶.
Do two prompts, and then‚Ä¶. That's the solution, right? So, yeah, really interesting, a good take, but I'm just wondering what are your thoughts on this?
The more parameters you give it, the tighter it is. I don't know, yeah.
Sure, sure.
</details>
</details>

<details>
  <summary>09:38:56</summary>

### 1. **Automation vs. Augmentation**
- AI is more effective at synthesizing and generating content than fully automating complex workflows.
- Full automation remains brittle, especially when dependent on multiple AI modules each prone to error.
- Most jobs consist of diverse tasks, some suitable for automation, but many better suited for augmentation.

### 2. **Heuristic Evaluation with AI**
- AI models like VLMs (e.g., Google Gemini) can critique UI designs using known principles (e.g., Nielsen‚Äôs heuristics).
- These models can identify usability issues but still require human oversight and prioritization.

### 3. **Vibe Coding and Prompt Engineering**
- Tools like Figma‚Äôs vibe coding features can generate hi-fi mockups based on descriptive prompts.
- Concerns include difficulty in fine-tuning outputs and the risk of presenting high-fidelity visuals too early, leading to biased feedback.

### 4. **Blurred Roles Across Teams**
- AI tools are enabling PMs to create prototypes, traditionally the domain of designers.
- This can cause confusion or overattachment to design artifacts not reviewed by design teams.

### 5. **Collaborative and Responsible AI Use**
- Suggestions include marking AI-generated designs to prevent miscommunication within teams.
- Designers can embrace a guiding role, helping others use AI tools within brand and UX standards.

### 6. **Design Democratization**
- With AI‚Äôs assistance, roles are blurring between PMs, designers, and engineers.
- While this increases accessibility, it also demands clear communication protocols and internal guidelines.

## Conclusion:
The discussion highlights a shift toward collaborative design environments where AI is a tool for empowerment rather than replacement. Participants emphasize the importance of shared design languages, proper prompt use, and ensuring AI outputs are contextualized and responsibly used.

<details>
  <summary>09:38:56 transcript</summary>

One more, maybe Ramya, and then I want to pop back, uh, to some of these questions about.
About that phenomenon, because I think it's really interesting. Ramya, do you want to jump in?

09:39:10
Sure, yeah. Um, I was just interested, uh, when Paul talked about the whole, uh, round and sharp edges, and how we have some of these best practices in kind of designing AI products.
But on the flip side, I was also thinking about, um, where‚Ä¶ where and how we can apply the same, uh, when‚Ä¶.
In kind of using AI tools to design, um, you know, products. So, uh, if you have any kind of thoughts, suggestions on, um, where.
Kind of already are people in the market kind of using AI tools, uh, to kind of automate a lot of these workflows, or, you know, um, are these kind of, um, upward trending, or any research or‚Ä¶.
Thoughts on that? I think, uh, I was just curious and thinking out loud on those terms.

09:39:53
Yeah. Yeah, good question. I think‚Ä¶.
I think that a lot of people are using AI a little bit right now.
Which is to say, they're sort of‚Ä¶ the things that it seems to be very good at is.
Synthesizing information together. And‚Ä¶ also in the generative capacity, sort of.
You may have seen there are various papers and so on about, like, AIs as brainstorming partners and things like this, um.
We can talk about that later if you want. The literature seems to suggest that it actually reduces.
Brainstorming diversity, which is interesting. Um, the‚Ä¶.
I feel like‚Ä¶. There are some people‚Ä¶ I've not seen that many cases where people automate their whole job.
Uh, and‚Ä¶. It's‚Ä¶ I think it's because I think often people.
Like, okay, take my job. Like, I do a lot of different things. Some parts of my job‚Ä¶.
I would happily automate, like‚Ä¶. Grading. I would love to do less grading, um, but the, like, the FaceTime with students, I wouldn't want to reduce that.
The, um‚Ä¶ maybe how to help a troubled student? I would love, you know, sort of, uh, Jiminy Cricket on my shoulder, helping me figure out, like, how to‚Ä¶ how to help.
Best coach this student, but I think that, like, most jobs are this sort of, like, constellation of many, many different tasks, and some subset of them.
Can sort of be fully augmented‚Ä¶. Automated, but not very many. I also just think‚Ä¶.
If you look at the sort of all of the smoke around Agentic AI and, like, these workflows and so on, they haven't really‚Ä¶ I think if there were‚Ä¶.
If there were real fire underneath it, it would have taken over more by now. I think that what people are finding is.
That they're often‚Ä¶ they're just pretty brittle. Like, it's sort of like a game of telephone, where, like, you know.


09:41:50
This AI module calls that one, calls that one, calls that one, and if any single one of them, they're all sharp-edge problems. So if any single one of them.
Make an error, which is just, like, literally multiply 1 minus the probability of each individual one making an error.
Like, then the whole thing falls apart. So, I think that in many cases, these, like, full automation solutions‚Ä¶ look.
It works in some cases, like, we can now identify dogs and cats in images in almost a fully automated way, but it turns out that‚Ä¶.
Jobs don't just identify dogs versus cats in images, they're sort of much‚Ä¶.
Richer information tasks that I think, you know, we will get there in some cases, but I think it's going to be a slower advance.
I want to, um‚Ä¶ yeah, go ahead, sorry.

09:42:30
Yep. No, uh, thanks for sharing that, um, because I was just curious if, you know, there will be a time, or even if just for the case of‚Ä¶ just for the purpose of curiosity, you know, can try and build.
Something, uh, with all the different sub, uh, processes or items that we work.
In design to build a product, if all of that we can, you know, try and automate and see how that result is versus how, uh, humans' effort is in creating the same kind of product is. But I just got to kind of.
Thinking on those terms when you talked about the whole reference sharp edges. So yeah, I was just curious to understand and.
Learn more.

09:43:07
I think‚Ä¶ I think they're‚Ä¶ well, okay, let's dig into one of these, actually. I think I have‚Ä¶ yeah, okay, so this is the one that I was actually, um, just mentioning. I think it was at‚Ä¶.
Joey, I was deferring. So, okay, a few things about, like, design and AI. I think there's, like, lots of interesting questions here.
So here's a project I want to show, um‚Ä¶. From UC Berkeley, that came out at the very end of last year.
Um, where essentially what they're finding is that these VLMs. The V is vision, so it's like an LLM, but now it can take images as well, like ChatGPT can do this.
If you give it an interface, and you give it a design.
Uh, like, a critique? A set of lenses from which to critique, whether that would be accessibility guidelines, or here, um, Nielsen's heuristics.
It can provide‚Ä¶. Those critiques.
Fairly accurately. So, this is interesting to me, because this is a thing that, like, most designers learn to do in sort of their first course.
Is, you know, here's a set of design guidelines, now please, you know, please critique.
I believe these ones were done using‚Ä¶ there's a question what VLM was‚Ä¶ I think this is‚Ä¶.
Google Gemini, because this was a team that was collaborated with Google Research, if I remember correctly. But I promise you, if you try this on, like, you know.
03 on ChatGPT, it'll work as well. Um, the only thing that I think they did manually, if I remember correctly, is you can see that it's got these blue.
Overlays like they trained it to actually highlight the part of the image that it's trying to call out.
Um, so I think this is interesting. Because if you've‚Ä¶.
Think of it as a heuristic evaluation machine. Um‚Ä¶.
You know, you don't expect a heuristic evaluation to catch everything. It's like a‚Ä¶ it's a decent metaphor, right? It catches a lot of stuff, but not everything, and not everything that it catches is worth fixing.
Right? Because there may be other. Kinds of trade-offs you're trying to manage.
But I think this is actually pretty powerful, and as we'll talk about later in the course, we've been developing some technology that can actually even sort of simulate things that users might click on, or where they might get stuck.
Like, that could be very powerful. Um.
Maybe even more spicy than that. I think there's this whole space of what's being called vibecoding, right? Where essentially.
You know, I'm just grabbing one such tool here, but, you know, Figma had one of these, has one of these, but there's a lot where you sort of‚Ä¶.
Describe what it is you're looking for, and it produces. A, like, fairly‚Ä¶ what we'd call sort of, like, standard design, meaning that it follows, kind of, standard design patterns.
Um, and‚Ä¶. You know, in many ways.
Can produce something pretty plausible. Now, you'll notice there that it was, like, not very consistent across screens, but.
It's interesting. It's, you know, it's probably solid, but what I would say is that these things are decently‚Ä¶.
Able to produce hi-fi mocks that follow design patterns. But, the kinds of trade-offs we make is that they're‚Ä¶ right now, they're very difficult to control. Like, if you just want to change this one button about it, good luck.
Um, and this is fairly fundamental to how these diffusion models work.
That there's not strong‚Ä¶ like, you can mask certain parts of it. It's just very difficult to get the models to do these things. It's‚Ä¶.
It's not on the short-term trajectory that it's going to be very controllable, where you can just sort of, like, grab a part of it and drag it around, or grab it and, you know, highlight it and change it. We will eventually get there, I suspect, but it's‚Ä¶.
Still difficult. I also think that there's a big risk here.
Where it means that people are creating high-fidelity mocks where they should have been producing something low fidelity, like a sketch.
And we know from a lot of the‚Ä¶. You know, training and research here that if you show people things that look really pretty, like these designs, when what you have is.
Actually, uh, sort of an early concept, you're gonna get feedback on, like, the layout and the colors and the fonts, instead of feedback on the idea itself.
And so I do worry. About people using this. I do think that this basically means that there's going to be a lot of.
What I'm going to call PMs doing design. I think this kind of tool does enable a lot of PMs to probably produce a first.
Sort of visualization of the kind of thing that they're trying to go for, which might wind up improving communication with the design team.
But I do worry a little bit about it. I mean, again, there are lots of different applications of this.
Um, here's another one, Replit. You know, I just wanted to have a little bit of a conversation about this, because I think when we talk about, like, is AI going to displace jobs, or things like this.
I mean, I think it's worth talking about, is it going to displace design.
I mean, as you can tell, my sense is. Probably not, although it might mean, like, if we're talking about, like, again, if PMs are doing what entry-level designers might have done, it might displace some.
Some entry points for, again, these entry-level designers. On the other hand, it probably means that.
Design is going to get more diffused throughout many organizations, where everyone's expected to sort of have a fairly plausible.
First UI design for their system. Udit, am I‚Ä¶ am I pronouncing your name correctly?

09:48:29
Uh, yes, thanks. So in our organization, we are using wipe coding pretty extensively for producing this, uh, these prototypes.
My entire design team, I'm pushing them towards it. So there are several steps we do before that, and coming up with the right prompt, like, two-page long prompt, before you even get to.
By coding, and then even after that, after a couple of iterations, we are moving from, let's say, Lavable or any of those tools to a full-fledged.
Ai-enabled IDE, so that allows us to iterate over it, so not just the initial.
Prototype, but do, um, pinpointed exact changes that you want to do any trade from there on, and making it.
Dev-ready, like, handoff ready. So we are trying to explore that. To your point, whether it'll replace the designers, probably not, but what it is doing is it is blurring the lines between the product managers and the‚Ä¶ what designers are doing, and what designers are doing, and what devs are doing.

09:49:13
Yeah, I mean, I think that, obviously, in a large company, right, you have.
09:49:27
So those lines are, uh, are getting blurred, is what I see.

09:49:36
A difference between the people who do, like. Information design.
You know, product design, concept design, and then, like, UI design, which often, like, those are different roles, and one thing that's interesting is that these tools are sort of, like.
Just doing it all together in a way that maybe you didn't actually want to focus on this part at all, and yet now suddenly you have a very specific picture of a cat.
That, like, it would have been better if it had just sort of wireframed and been, like, picture of cat.
Right? So I‚Ä¶. I think it's gonna be an interesting ride. Uh, Nanad, did you want to jump in?

09:50:12
Uh, yeah, thanks. I, uh, I just wanted to kind of‚Ä¶ and forgive me, everyone, I won't shut up about it, because, uh, it's‚Ä¶.
It's something that's been on my mind ever since, uh‚Ä¶. Ever since I've seen Rabbit R1, uh, and it's kind of‚Ä¶ I don't know, like, uh‚Ä¶.
If the terminology is right, like the large action model that sort of also kind of.
You know, processes, uh, the UI, like, in the background. Even though it sort of seems like a kind of a grafted solution, but what was interesting.
Interesting to me is how it kind of‚Ä¶. Adapted the UI itself.
Uh, as a kind of a, you know. Response, so‚Ä¶.
To your point, and just kind of circling back to, uh, how you said, like, that would influence apps themselves.
I really found that intriguing, and even though it seems like, you know, a surface-level.
Um, you know, topic. Uh, you know, like, buttons and sort of, like, widgets, kind of.
Appearing out of thin air and just kind of getting. Getting, uh, kind of designed on the fly by the model itself, I‚Ä¶.
I really found that really, uh, exciting, so I just want to, uh, you know, your thoughts on that, and like, uh, was there anything to‚Ä¶.
To our one, uh, as a kind of a, you know, product, even though it's sort of‚Ä¶.
Um, was discovered that it's basically like a glorified Android app that, uh, is just sort of, you know, went a little under.

09:51:40
So this is‚Ä¶ you're not talking about the deep research model, you're talking about‚Ä¶ was it the rabbit R?
09:51:46
The little‚Ä¶ yup, yeah, Rabbit R1, the little‚Ä¶. Kind of gadget companion that was sort of, you know, uh‚Ä¶.
09:51:53
Yeah, okay, so it also reminds me of the Humane IPIN, I don't know if you tracked that.

09:52:00
Okay, well, I think I've a number of UX comments on those things. I mean.
09:52:05
I don't know, please, if‚Ä¶.

09:52:06
I mean, for one, I think that, look, there's just a technology issue here where.
The models aren't fast enough, so there's a lot of, like, you ask a question, then you're sitting there awkwardly in front of your friend, waiting for 10 seconds while it sits there and thinks.
Um, you know, there are some tasks for which waiting is fine. For other ones, I think, again.
People don't know what it's good for yet, so go back to the Google Glasses way back when, right? It's‚Ä¶.
There was this huge vision of augmented reality and what it's gonna‚Ä¶ but you know what? Like, it turns out we don't really know what problems that's solving for people yet.
And so having sort of this, like, wearable AI thing is seen as, like, you know, a big privacy intrusion, but without.
Providing much benefit. I think, in general, people are willing to trade off privacy for benefit.
If, you know, but‚Ä¶. What‚Ä¶ where was the benefit? It just wasn't a big enough win.
Yet. And so, I view it as sort of being early days for those kinds of things.

09:53:07
Uh, we can talk about it in a future conversation, uh‚Ä¶.
I have some ideas here. Like, I do think that the future of user modeling is going to get really interesting. We have a project I can talk about next time where we're, like, just.
By‚Ä¶ if you privately watch yourself on your computer, you allow a little private model to watch yourself. We can build this really rich notion of who you are.
And, like, now imagine that any application that you're designing could ask, you know, a sort of.
Freely ask a pretend version of the user, hey, would Michael like it if I made this suggestion? Would Michael like it if I did this?
And without bothering Michael ever, it can sort of figure out what to push on. I think that could be very powerful.
Uh, Peter, over to you.

09:53:47
Hi, yeah, you raised a really good point that I hadn't really considered, but I've run into, of‚Ä¶.
You know, all these people who are not designers having this ability to now design prototypes, design webpages really quickly, we had a situation I ran into where our CTO.
Wanted to make a really simple interface. You pick a value, it would tell you approximately how much it's gonna cost.
But the values range from $1,000 to $100 million. And he built a slider widget, because he loves slider widgets, and now.
Because he has this interactive prototype. That he really is kind of obsessed with it, because he sees it like, oh, this works really cool, I really like the look of it.
And because he could very easily produce an interactive prototype, he kind of fell in love with the design, even though, as a designer, I know.
I'm not gonna have you pick a value with a slider from 1,000 to 100 million.
So, it kind of‚Ä¶ it‚Ä¶. It's an interesting problem, I think we're gonna start running into more and more, where PMs and‚Ä¶.
You know, people of that ilk can start building prototypes and doing designs really easily.
And we're gonna have to develop a whole new set of soft skills for kind of negotiating that‚Ä¶ that space.

09:54:52
Yeah, so, okay, my con‚Ä¶ this doesn't exist yet, but it's the kind of thing that we've been building on the research side, and again, we'll talk about it a little later in the course, it's called generative agents.
Your response then is, I'm gonna send a million. Customers of your product into the system, and look at all of the alarm bells that go off, where they all get super frustrated at this thing. I've found that over and over again, it winds up being‚Ä¶.
That, like, people sort of respond viscerally to, like. Just people fuming at not, like, they're sitting there staring at the interface, and they do not know what to do. Like, I think that there‚Ä¶ there will be an equilibrium here.
But it may take some time to‚Ä¶. To rough out. I mean, I think we can't just sort of unilaterally be like, oh, those people, they're so silly. Insofar as, like, you know.
The same thing happened with travel agents, right? We, like, humans used to use travel agents all the time.
And now, you know, you use Google Flights, or whatever. Right? Like, and we're probably not as good as the travel agents work. But, on the other hand.
For, you know, 90% of cases, you don't need something as good as a travel agent.
Um, and I think travel agents are an interesting one, because, well, yeah, actually, there are fewer of them around now.
Now, will that happen to designers? Gosh, I‚Ä¶. I'm guessing not?
But I think we'll‚Ä¶. A little bit of time will tell, but that's where I'm putting my money.
Deepdi, we have‚Ä¶ okay, so we have a few minutes left, so let's try to hit maybe some quick ones, so Deep Deep Crystal and Francis, we'll see how far we can get.

09:56:28
Yeah, no, I am a product manager, and I've been seeing the conversations on the side, so‚Ä¶.
One thing I‚Ä¶ you know, I feel it's very organization-dependent, so in my company, Intuit.
We use prototyping just to make sure that designers understand our vision. It is not to say, hey, we are going to replace designers entirely. Having said that.
There are some, you know, small startups wherein PMs are expected to build that, uh, you know, sophisticated level of designing that then designers can just pick up.
So, it all depends on, you know, what is, like, how does your organization, like, is it structured, and how do they support your, you know.
Pm as a skill set versus design as a skill set.

09:57:13
Yeah, I mean, I think that it's, you know, great point.
I think it's going to diffuse‚Ä¶ let me turn it around. I think that also what's going to happen is that now it's going to empower designers to create interactive code.


09:57:28
Right? Hey, I created this, now look, you can see exactly how this is supposed to work. You can't tell me that you can't implement this, because look, it's working.
Right? So I think it's gonna actually, you know, blur the boundary in both directions, which will be an interesting time.


09:57:41
So, again, I think we'll have to wait and see, but that's‚Ä¶.
I'm curious. Like, who's really actually gaining more power in this relationship? We'll find out.
Uh, Crystal.
Oh, where did Crystal go? Oh, there. Okay, go ahead.

09:58:00
Hi. Hi there. Um. Thank you so much for this talk, it's so illuminating, um, and this chat has been so‚Ä¶.
Uh, mind-bending, so, um, I guess my only contribution was thinking that.
Um, when I was a junior designer, I thought, like, oh, I have to, like, protect my job, I do these things, I make the pretty pixels.
The more advanced and senior I got, though, with the years.
Understanding that PMs have a part to play in the design‚Ä¶ like, everyone has a part to play in the best UX.
For the end user, and everyone has these different parts of the image, the bigger picture.
For the user, and so I would like AI to have more, like, shared information for.
All people in the company to make the best decisions when they're working.
And I'd also like to see maybe creative guidelines internally in the company. So if a PM makes something.
You know, maybe don't make it look so, um, high fidelity that the CEO thinks the designers did this.
So there's, like, implied, like, uh, things that people might start assuming, all these assumptions internally, and then confusing the CEO, or confusing.
The developer, like, clearly, they must have talked to the designer, it would not have looked this pretty.
You know, like, making it clear where‚Ä¶ how did you generate these images? Maybe having, like, watermarks, or make sure it exports in black and white.
Across the company, or only use cat images when there is a photo feed. Like, something that shows that this is not.
The end-end, but I don't think that for designers, gatekeeping design makes them very popular with other people in the organization.
If anything, designers can be that forefront of, like, great job, that's amazing! They could be, like, the executor's.
Of good design, but then also, like, learn how to put that hamburger of, like.


09:59:57
Cool concept, I see where you're going with this. Sliders, maybe not the best UI solution, but we could do some heuristic evaluation. Let's jump on a side chat and talk about it.

10:00:09
I also could see design‚Ä¶. The design org as owning tools that would empower PMs to do better jobs the first time.
So imagine down the road that it would be possible to create almost, like.
Modules that ensure that whatever the PM creates is, like, within the design language of.
Whatever your product is, or I can say, like, oh, this‚Ä¶ use this because it'll produce a lo-fi mock, or something like this, to sort of help.
Guide the company toward producing the right kinds of. Mocks and prototypes at the right time.
Okay, 10 o'clock, Francis, do you think you can do it quickly?
</details>
</details>


<details>
  <summary>10:00:45</summary

**Focus:** Vibe Coding and its Potential for Consumer Innovation

The discussion explored the concept of **vibe coding**, a term referring to how non-technical users are now creating their own AI-powered tools using intuitive, natural language interfaces.

### What Is Vibe Coding?
- A process where users describe what they want, and AI generates functional code or prototypes.
- Empowers **non-developers** to build small, useful tools without needing full programming knowledge.

### Consumer Potential
- Tied to the idea of **end-user innovation** (Eric von Hippel, MIT Sloan): people creating solutions for their own needs.
- Examples include:
  - Personal scheduling apps
  - Lightweight automation tools
  - Throwaway programs solving niche or one-time tasks

> These are **"micro-tools"**‚Äîsmall, personalized solutions that are easy to make and useful in daily life.

### Practical Example
The speaker shared that they exported a CSV of meeting availability and used ChatGPT to generate a scheduling script that could be rerun with updates. This "vibe-coded" solution saved time and effort without requiring software engineering.

## Takeaway

Vibe coding has **tremendous potential for consumer empowerment**. While enterprise-scale applications still require deep expertise and infrastructure, **the rise of AI-generated micro-tools may redefine how everyday people solve problems** using technology.

**This represents a shift toward democratized software creation**‚Äîenabling everyone to become a builder.

<details>
  <summary>10:00:45 transcript</summary>

Yes, just a quick question to you on your, uh, thoughts on vibe coding.

10:00:51
From a consumer standpoint, there have been a lot of articles about.
Um, consumers or people just coming up with their own solutions, their own AI solutions, by vibe coding.
And so I wanted to ask what your thought. On how‚Ä¶ how popular this could be with consumers, and where that.

10:01:13
Could lead.

10:01:14
Right, so I think the thing that is really interesting to me about it is what Eric von Hippel at MIT Sloan calls, like.
End-user innovation. Imagine now everyone in the, sort of, in the shed in their garage, like, able to create, like, little customized solutions for the little one-off tasks they have.
I see a huge proliferation of that happening, because they're like little micro-tools.
I think that's within, you know, spitting distance. Building these really large enterprise-scale applications with AI, I think that's much further away.
But I think these little micro-tools, I think, really does seem feasible, and I've done that, like, I needed to schedule all of my student meetings for the summer, and I, like, had a when to meet, I exported a CSV of when people were free, and I asked ChatGPT to just help me, and it actually wrote a little program, so that when the availability changed, I just ran it again.
That's like a, you know, make it and throw it away kind of program. That kind of thing I could see being really a helpful, and B, actually doable these days.
</details>
</details>